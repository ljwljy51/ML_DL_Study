{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xhvb7TZcaYz"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaaV4vdqXsJe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2TmyQX6RK96"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive/DACON_Practice/TourismData_MultiModal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg29SCTnRK91"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, TFAutoModel, AdamWeightDecay, AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "#초기화되지 않은 weight가 있거나, 불러왔는데 사용을 하지 않는 weight가 존재할 때 발생하는 Warning 안뜨도록 함\n",
        "\n",
        "def my_seed_everywhere(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "my_seed_everywhere(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrTEq9AHRK96"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation -> 추가 학습용 데이터, 검증 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_voBJIbcRK97"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv') #train 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB-0-i2yRK98"
      },
      "outputs": [],
      "source": [
        "#증강에 사용할 형용사, 부사, 유의어 모음 불러오기(Google Drive 첨부 - https://drive.google.com/drive/folders/1U6Y6Zv_PxZXsXLgLlgzu6smZ_e_b2mpS?usp=sharing)\n",
        "\n",
        "adj_ls = pd.read_csv('adjective.csv')['adjective'].to_list() #형용사\n",
        "ad_ls = pd.read_csv('adverb.csv')['adverb'].to_list()  #부사\n",
        "\n",
        "main_token = pd.read_csv('sim_data.csv')['token'].to_list() #유의어 기준 단어 리스트\n",
        "sub_token = pd.read_csv('sim_data.csv')['sim_token'].to_list() #대응 유의어 모음 리스트\n",
        "sim_dic = {} #딕셔너리에 저장\n",
        "for m, s in zip(main_token,sub_token) : #메인, subtoken받아 딕셔너리 형태로 만듦\n",
        "    sim_dic[m] = s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmhbpHA3RK98"
      },
      "outputs": [],
      "source": [
        "#형용사 증강 : 문장의 명사 앞에 랜덤으로 형용사어 삽입 함수\n",
        "\n",
        "def aug_adj(senten):\n",
        "    tokens = senten.split(' ') #문장을 입력으로 받아 띄어쓰기를 기준으로 나눠 토큰 구함\n",
        "\n",
        "\n",
        "    cnt = 0 #수\n",
        "    token_ls = [] #명사 포함한 토큰 담을 리스트\n",
        "    for i in range(len(tokens)) :\n",
        "        token = tokens[i] #각 단어로부터 토큰 구하기 위함\n",
        "        if token != '' : #공백이 아닌 경우\n",
        "            if token[-1] == '은' or token[-1] == '는' or token[-1] == '이' or token[-1] == '가' : #주어인 경우\n",
        "                cnt +=1 #개수 추가\n",
        "                token_ls.append(token) #토큰 추가\n",
        "            if token[-1] == '을' or token[-1] == '를' : #목적어\n",
        "                cnt +=1\n",
        "                token_ls.append(token)\n",
        "\n",
        "    if cnt > 0 : #감지된 토큰 있을 경우\n",
        "        for i in range(random.randint(1,cnt)) : #특정 개수만큼의 명사 토큰에 대해 형용사 붙여 증강\n",
        "            token_index = random.randint(0,cnt-1)\n",
        "            senten = senten.replace(token_ls[token_index],'{} {}'.format(adj_ls[random.randint(0,len(adj_ls)-1)],token_ls[token_index]))\n",
        "\n",
        "    return senten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wSo3Dz_RK98"
      },
      "outputs": [],
      "source": [
        "#부사 증강 : 문장의 동사 앞에 랜덤으로 부사어 삽입 함수\n",
        "def aug_ad(senten):\n",
        "    tokens = senten.split(' ')\n",
        "\n",
        "\n",
        "    cnt = 0\n",
        "    token_ls = []\n",
        "    for i in range(len(tokens)) :\n",
        "        token = tokens[i]\n",
        "        if token != '' :\n",
        "            if token[-1] == '.':\n",
        "                cnt +=1\n",
        "                token_ls.append(token)\n",
        "\n",
        "    if cnt > 0 :\n",
        "        for i in range(random.randint(1,cnt)) :\n",
        "            token_index = random.randint(0,cnt-1)\n",
        "            senten = senten.replace(token_ls[token_index],'{} {}'.format(ad_ls[random.randint(0,len(ad_ls)-1)],token_ls[token_index]))\n",
        "\n",
        "    return senten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMOM5W9zRK99"
      },
      "outputs": [],
      "source": [
        "#유의어 증강 : 문장의 단어들을 유의어로 변경하는 함수\n",
        "def aug_simul(senten):\n",
        "    tokens = senten.split(' ')\n",
        "    cnt3 = 0\n",
        "    token_ls3 = []\n",
        "    for i in range(len(tokens)) :\n",
        "        token = tokens[i]\n",
        "        if token in sim_dic.keys(): #유사어 존재할때\n",
        "            token_ls3.append(token)\n",
        "            cnt3 += 1\n",
        "\n",
        "    if cnt3 > 0 :\n",
        "        for i in range(random.randint(1,cnt3)) :\n",
        "            token_index = random.randint(0,len(token_ls3)-1)\n",
        "            to_index = tokens.index(token_ls3[token_index])\n",
        "            to_val = sim_dic[tokens[to_index]].split('/')\n",
        "            if len(to_val) > 1 :\n",
        "                tokens[to_index] = to_val[random.randint(0,len(to_val)-1)]\n",
        "            else :\n",
        "                tokens[to_index] = to_val[0]\n",
        "\n",
        "            token_ls3.pop(token_index) #증강 이뤄진 경우 다시 증강 이뤄지지 않도록\n",
        "\n",
        "\n",
        "    senten = ' '.join(tokens)\n",
        "    return senten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fF_0dsHRK9-"
      },
      "outputs": [],
      "source": [
        "#이미지 증강\n",
        "\n",
        "\n",
        "def img_aug(img) :\n",
        "    loader_transform1 = transforms.ColorJitter(\n",
        "        brightness=0.6,\n",
        "        contrast=0.6,\n",
        "        saturation=0.6,\n",
        "        hue=0.2\n",
        "        )\n",
        "    loader_transform2 = transforms.RandomAffine(degrees=90,fill=0)\n",
        "\n",
        "\n",
        "    imgArray = np.array(img) # 이미지 분석을 위해 배열 전환\n",
        "    img_shape = imgArray.shape #기존 shape 받아와 변형\n",
        "    x = int(img_shape[0] * 0.8)\n",
        "    y = int(img_shape[1] * 0.8)\n",
        "    x2 = int(img_shape[0] * 0.4)\n",
        "    y2 = int(img_shape[1] * 0.4)\n",
        "\n",
        "    re_img = loader_transform1(img) #ColorJitter\n",
        "    re_img = loader_transform2(re_img) #RandomAffine\n",
        "    re_img = transforms.RandomCrop(size=(x,y))(re_img) #RandomCrop\n",
        "    re_img\n",
        "\n",
        "    imgArray_aug = np.array(re_img) #이미지를 배열로 변환\n",
        "    indx1 = random.sample(range(x2),2) #Random 노이즈 부여 위함\n",
        "    indx2 = random.sample(range(y2),2)\n",
        "\n",
        "    length = (np.max(indx1)-np.min(indx1))*(np.max(indx2)-np.min(indx2))*3 #총 노이즈 길이\n",
        "\n",
        "    imgArray_aug[np.min(indx1):np.max(indx1), np.min(indx2):np.max(indx2), range(3)] = np.random.choice(256, length, replace=True).reshape(((np.max(indx1)-np.min(indx1)),(np.max(indx2)-np.min(indx2)),3)) #length만큼의 노이즈 추출해 reshape해서 원본 이미지 값 수정\n",
        "\n",
        "    aug_img = Image.fromarray(imgArray_aug) #이미지 배열을 다시 이미지 객체로 변환\n",
        "\n",
        "    return aug_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYlLMU8nRK9-"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('image/val')   #증강 이미지를 저장할 폴더 생성\n",
        "\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sam_overview = train['overview'].to_list() #텍스트후기\n",
        "sam_label = train['cat3'].to_list() #'소'카테고리 정보\n",
        "sam_img = train['img_path'].to_list() #이미지 경로\n",
        "\n",
        "\n",
        "#증강 샘플 1개 생성\n",
        "sample = sam_overview[0]\n",
        "label = sam_label[0]\n",
        "img = Image.open(sam_img[0])\n",
        "\n",
        "new_sample = aug_adj(sample) #형용사 증강\n",
        "new_sample = aug_ad(new_sample) #부사 증강\n",
        "new_sample = aug_simul(new_sample) #이미지 증강\n",
        "\n",
        "new_img = img_aug(img)\n",
        "save_path = 'image/val/val_1.jpg'\n",
        "new_img.save(save_path,'JPEG') #validation 디렉토리에 증강된 이미지 저장\n",
        "\n",
        "val_set = pd.DataFrame({'img_path' : [save_path], 'overview' : [new_sample], 'cat3' : [label]}) #증강된 데이터를 csv파일 형태로 만들기 위함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmWdtQYQRK9_"
      },
      "outputs": [],
      "source": [
        "#미리 만들어둔 샘플 1개와 concat하여 5000개의 증강데이터 생성\n",
        "for i in range(4999) :\n",
        "    idx = random.randint(0,len(sam_overview)-1)\n",
        "    sample = sam_overview[idx]\n",
        "    label = sam_label[idx]\n",
        "    img = Image.open(sam_img[idx])\n",
        "\n",
        "\n",
        "    new_sample = aug_adj(sample)\n",
        "    new_sample = aug_ad(new_sample)\n",
        "    new_sample = aug_simul(new_sample)\n",
        "\n",
        "    new_img = img_aug(img)\n",
        "    save_path = 'image/val/val_{}.jpg'.format(i+2)\n",
        "    new_img.save(save_path,'JPEG')\n",
        "\n",
        "    df = pd.DataFrame({'img_path' : [save_path], 'overview' : [new_sample], 'cat3' : [label]})\n",
        "    val_set = pd.concat([val_set,df],axis=0)\n",
        "\n",
        "#총 5000개의 증강 데이터가 생성됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDxf4sPmRK9_"
      },
      "outputs": [],
      "source": [
        "val_set.to_csv('val_set.csv')\n",
        "del val_set\n",
        "# 증강데이터에 대한 csv파일까지 생성 완료"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we_zbubCRK9_"
      },
      "outputs": [],
      "source": [
        "# 텍스트 증강을 통한 샘플 불균형 완화\n",
        "def text_aug(label_ls, train_set, num, adj_aug=True, ad_aug=True, sim_aug=True ):\n",
        "    train2 = train_set\n",
        "    cnt2 = 16985 #훈련데이터 수\n",
        "    for i in label_ls : #각 레이블정보마다 돌면서 확인\n",
        "        cnt = Counter(train2['cat3'])[i] #각 레이블 수 추출\n",
        "        if cnt < num : #레이블 수가 특정 수보다 작으면\n",
        "            for j in range(num-cnt): #필요한 만큼\n",
        "                print(j) #몇 번째 데이터 증강인지 확인하기 위함\n",
        "                cnt2 += 1 #훈련데이터 수 증가\n",
        "                df3 = train2.loc[(train['cat3'] == i)] #해당 레이블을 갖는 데이터를 찾아 df3으로 추출해옴\n",
        "                df3_ls = df3['overview'].to_list() #df3에서의 텍스트 후기 데이터\n",
        "                num1 = len(df3_ls)\n",
        "                idx = random.randint(0, num1-1) #특정 데이터에 대해\n",
        "                sample = df3_ls[idx]\n",
        "                if adj_aug == True : #각 속성별로 적용 여부 체크해 데이터 증강 적용\n",
        "                    sample = aug_adj(sample)\n",
        "                if ad_aug == True :\n",
        "                    sample = aug_ad(sample)\n",
        "                if sim_aug == True :\n",
        "                    sample = aug_simul(sample)\n",
        "\n",
        "\n",
        "                df4 = pd.DataFrame({'id' : ['TRAIN_{}'.format(cnt2)], 'overview' : [sample], 'cat3' : [i]})  #증강된 데이터 정보를 데이터프레임화해 기존 데이터에 병합 (데이터 불균형 해소)\n",
        "                train2 = pd.concat([train2,df4],axis=0) #행방향으로 합침\n",
        "    return train2 #샘플 뷸균형을 해소한 훈련데이터 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIdssiD8RK-A"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv') #훈련데이터 로드\n",
        "\n",
        "train_set = train[['id', 'overview','cat3']] #전체 데이터 중 id, overview, 소분류 정보만 사용\n",
        "label_ls = list(train_set['cat3'].unique()) #유니크한 label값만 뽑아옴\n",
        "\n",
        "#형용사만 사용 / 최소 50개\n",
        "data1 = text_aug(label_ls,train_set,50,True,False,False) #형용사 증강만 적용한 데이터. 각 레이블 당 데이터가 최소 50개는 존재하도록 함\n",
        "data1.to_csv('train(adj,50).csv', index=False)\n",
        "del data1\n",
        "\n",
        "#부사만 사용 / 최소 50개\n",
        "data2 = text_aug(label_ls,train_set,50,False,True,False) #부사 증강만 적용한 데이터\n",
        "data2.to_csv('train(ad,50).csv', index=False)\n",
        "del data2\n",
        "\n",
        "#유의어만 사용 / 최소 50개\n",
        "data3 = text_aug(label_ls,train_set,50,False,False,True) #유의어 증강만 적용한 데이터\n",
        "data3.to_csv('train(sim,50).csv', index=False)\n",
        "del data3\n",
        "\n",
        "#형용사+부사 / 최소 50개\n",
        "data4 = text_aug(label_ls,train_set,50,True,True,False) #형용사, 부사 증강 적용 데이터\n",
        "data4.to_csv('train(adj,ad,50).csv', index=False)\n",
        "del data4\n",
        "\n",
        "#형용사+부사+유의어 / 최소 100개\n",
        "data5 = text_aug(label_ls,train_set,100,True,True,True) #모든 증강을 적용한 데이터\n",
        "data5.to_csv('train(adj,ad,sim,50).csv', index=False)\n",
        "del data5\n",
        "\n",
        "del train_set\n",
        "del label_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkIO2U8vRK-A"
      },
      "outputs": [],
      "source": [
        "#model 학습\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#함수를 그래프로 컴파일하여 실행하도록 변경\n",
        "#대규모 모델 훈련과 같은 경우에는 그래프 모드(graph mode)로 실행하는 것이 더욱 효율적\n",
        "\n",
        "class SAMModel(tf.keras.Model): #keras 모델 상속받아 정의\n",
        "    def __init__(self, my_model, rho=0.05):\n",
        "        \"\"\"\n",
        "        p, q = 2 for optimal results as suggested in the paper\n",
        "        (Section 2)\n",
        "        \"\"\"\n",
        "        super(SAMModel, self).__init__()\n",
        "        self.my_model = my_model\n",
        "        self.rho = rho\n",
        "\n",
        "    def train_step(self, data):\n",
        "        (text, labels) = data #데이터로부터 text와 label분류\n",
        "        e_ws = [] #스케일이 ㅈ거용된 gradient값들을 담을 리스트\n",
        "        with tf.GradientTape() as tape: #Gradient를 계산할 수 있는 Tape를 생성. 연산을 기록하여 후에 Gradient를 계산하는 데 사용\n",
        "            predictions = self.my_model(text) #텍스트를 입력으로 해 prediction 구함\n",
        "            loss = self.compiled_loss(labels, predictions) #label과 비교해 loss 계산\n",
        "        trainable_params = self.my_model.trainable_variables #학습가능한 variable들 추출\n",
        "        gradients = tape.gradient(loss, trainable_params) #tape 객체 통해 구한 loss를 기반으로 파라미터 갱신\n",
        "        grad_norm = self._grad_norm(gradients) #Gradient의 크기 측정\n",
        "        scale = self.rho / (grad_norm + 1e-10) #계산된 gradient의 크기, rho를 통해 scale 변수 구함\n",
        "\n",
        "        for (grad, param) in zip(gradients, trainable_params): #각 gradient와 trainable_parameter를 대응시켜 스케일 적용\n",
        "            e_w = grad * scale #스케일 적용\n",
        "            param.assign_add(e_w) # e_w 값을 해당 파라미터에 더하고 업데이트. SAM 알고리즘에 따라 각 파라미터가 업데이트됨\n",
        "            e_ws.append(e_w) #각 파라미터에 적용된 e_w 값을 리스트 e_ws에 추가\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.my_model(text)\n",
        "            loss = self.compiled_loss(labels, predictions) #loss\n",
        "\n",
        "        sam_gradients = tape.gradient(loss, trainable_params) #sam 적용한 Gradient 계산\n",
        "        for (param, e_w) in zip(trainable_params, e_ws):\n",
        "            param.assign_sub(e_w) # SAM적용해 파라미터 갱신\n",
        "\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(sam_gradients, trainable_params)) #모델 파라미터 업데이트\n",
        "\n",
        "        self.compiled_metrics.update_state(labels, predictions) #모델 평가지표 업데이트\n",
        "        return {m.name: m.result() for m in self.metrics} #손실함수 이름, 값 딕셔너리 반환\n",
        "\n",
        "    def test_step(self, data):\n",
        "        (text, labels) = data #데이터 추출\n",
        "        predictions = self.my_model(text, training=False)\n",
        "        loss = self.compiled_loss(labels, predictions)\n",
        "        self.compiled_metrics.update_state(labels, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def _grad_norm(self, gradients):\n",
        "        norm = tf.norm(\n",
        "            tf.stack([\n",
        "                tf.norm(grad) for grad in gradients if grad is not None\n",
        "            ])\n",
        "        )\n",
        "        return norm\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass of SAM.\n",
        "        SAM delegates the forward pass call to the wrapped model.\n",
        "        Args:\n",
        "          inputs: Tensor. The model inputs.\n",
        "        Returns:\n",
        "          A Tensor, the outputs of the wrapped model for given `inputs`.\n",
        "        \"\"\"\n",
        "        return self.my_model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVFa6nQKRK-B"
      },
      "source": [
        "#텍스트모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vA-HQJWRK-C"
      },
      "outputs": [],
      "source": [
        "#텍스트 모델 전처리 함수\n",
        "def preprocessing(train,val,test, modelname, max_seq_len, add_token=False, emphasize_token=False, token_change = False):\n",
        "    encoder = LabelEncoder() #label encoder 객체 생성\n",
        "    y_train = train['cat3'] #train, val set으로부터 label 추출\n",
        "    y_val =  val['cat3']\n",
        "\n",
        "    y_train = encoder.fit_transform(y_train) #train data에 대한 label encoding\n",
        "    y_val = encoder.transform(y_val) #같은 값으로 encoding 위함\n",
        "\n",
        "    y_train_data = pd.Series(y_train) #series형으로 변환\n",
        "    y_val_data = pd.Series(y_val)\n",
        "\n",
        "    model_name = modelname\n",
        "    f = open('add_token.txt') #ex) 흑돼지->흑+돼지가 아닌 흑돼지로 인코딩되도록 하기 위함\n",
        "    add_token_ls = f.read().split()\n",
        "    emphasize_token_ls =  ['상설시장','채식주의','채식주의자','비건','비거니즘','고택','펜션','관아','팔각','주심포','건물','오일시장'] #반복을 통해 강조시켜줄 토큰\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name) #Tokenizer 객체 선언\n",
        "\n",
        "    if add_token == True :\n",
        "        for token in add_token_ls :\n",
        "            tokenizer.add_tokens(token) #토크나이저를 통해 토큰화해  append\n",
        "\n",
        "    def convert_examples_to_features(examples, labels, max_seq_len, tokenizer): #원본 input, label을 통해 bert, transformer에서 사용할 수 있도록 데이터 변환\n",
        "        #token_type_ids는 어떤 문장에 속하는지 나타내기 위함\n",
        "        input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
        "\n",
        "\n",
        "        for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
        "            if token_change == True :     # 특정 라벨에 특정 단어가 들어가는 경우 그 단어를 살작 변형 -> 사용했을때와 안했을때 모두 뽑아내서 앙상블\n",
        "                if label == '뮤지컬' :\n",
        "                    if example.find('뮤지컬') != -1 : #텍스트에 '뮤지컬'이라는 단어가 들어가는 경우\n",
        "                        example = example.replace('뮤지컬','뮤지컬(뮤지컬공연)') #단어 대체\n",
        "                if label == '분수' :\n",
        "                    if example.find('분수쇼') != -1 :\n",
        "                        example = example.replace('분수쇼','분수(분수쇼)')\n",
        "                if label == '채식전문점' :\n",
        "                    if example.find('채식') != -1 :\n",
        "                        example = example.replace('채식','채식(채식전문점)')\n",
        "                if label == '게스트하우스' :\n",
        "                    if example.find('게스트하우스') != -1 :\n",
        "                        example = example.replace('게스트하우스','게스트하우스(게하)')\n",
        "\n",
        "            #토큰 강조하기로 했을 경우\n",
        "            if emphasize_token == True :     #  특정 단어를 강조하기 위해 반복\n",
        "                for em in emphasize_token_ls :\n",
        "                    if example.find(em) != -1 :\n",
        "                        example = example.replace(em,'{} {} {}'.format(em,em,em)) #단어 반복\n",
        "\n",
        "\n",
        "            input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)      # 토크나이저를 통해 인코딩. 입력 토큰 사이즈 지정 및 패딩 적용\n",
        "            padding_count = input_id.count(tokenizer.pad_token_id) #pad_token_id는 패딩을 나타내는 토큰의 식별자(ID). 총 패딩 적용된 수 추출\n",
        "            attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count #패딩 적용 길이에 따라 attention mask생성\n",
        "            token_type_id = [0] * max_seq_len #모두 한 문장으로 이뤄져있음. 하나로 통일(?)\n",
        "\n",
        "            assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len) #input_id, attention_mask, token_type_id 길이 모두 동일한지 확인\n",
        "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "            assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            attention_masks.append(attention_mask)\n",
        "            token_type_ids.append(token_type_id)\n",
        "            data_labels.append(label) #변환된 정보 리스트에 저장\n",
        "\n",
        "        input_ids = np.array(input_ids, dtype=int) #리스트 정보를 array 자료형으로 변환\n",
        "        attention_masks = np.array(attention_masks, dtype=int)\n",
        "        token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "\n",
        "        data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "        return (input_ids, attention_masks, token_type_ids), data_labels #변환된 정보들 반환\n",
        "\n",
        "    y_test_data = [i for i in range(len(test))] #test data의 Y값 임의로 설정\n",
        "    x_train, y_train = convert_examples_to_features(train['overview'], y_train_data, max_seq_len=max_seq_len, tokenizer=tokenizer) #train data의 overview text정보를 통해 feature로 변환\n",
        "    x_val, y_val = convert_examples_to_features(val['overview'], y_val_data, max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "    x_test, _ = convert_examples_to_features(test['overview'], y_test_data, max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "    return x_train, y_train, x_val, y_val, x_test #변환된 train, validation, test set 정보 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sBeqq-DRK-C"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('weights')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "def train_inference(modelname, optimizer='Adam', dropout=0.1, TruncatedNormal=0.02, epocs=4, batch_size=8, sam = False, name = 'pred'):\n",
        "\n",
        "    class TFBertForSequenceClassification(tf.keras.Model):\n",
        "        def __init__(self, model_name):\n",
        "            super(TFBertForSequenceClassification, self).__init__()\n",
        "            self.bert = TFAutoModel.from_pretrained(modelname, from_pt=True) #모델 로드\n",
        "            self.dropout = tf.keras.layers.Dropout(dropout) #dropout 층\n",
        "            self.classifier = tf.keras.layers.Dense(128, #output 차원은 128\n",
        "                                                    kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02), #정규 분포에서 생성된 랜덤한 값을 사용하여 가중치(weight)를 초기화\n",
        "                                                    activation='softmax',\n",
        "                                                    name='classifier')\n",
        "\n",
        "        def call(self, inputs):\n",
        "            input_ids, attention_mask, token_type_ids = inputs #input으로부터 데이터 추출\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) #bert에 입력 넣어 output 추출\n",
        "            cls_token = outputs[1]\n",
        "            prediction = self.classifier(cls_token) #classifier layer 통과\n",
        "\n",
        "            return prediction\n",
        "\n",
        "\n",
        "    model = TFBertForSequenceClassification(modelname) #모델 객체 생성\n",
        "\n",
        "    if sam == True :\n",
        "        model = SAMModel(model) #SAM 적용할 경우\n",
        "\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy #label이 정수일 때 사용하는 loss\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=epocs, batch_size=batch_size, validation_data=(x_val,y_val))\n",
        "    model.save_weights('weights/{}'.format(name))\n",
        "\n",
        "    pred = model.predict(x_test, batch_size=batch_size) #테스트데이터에 대한 output 계산\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLg1JLc8RK-D"
      },
      "outputs": [],
      "source": [
        "#Text Model\n",
        "\n",
        "my_seed_everywhere(42)\n",
        "\n",
        "lr = 5e-6\n",
        "wd = 1e-2 * lr\n",
        "optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-6)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "val= pd.read_csv('val_set.csv')\n",
        "test = pd.read_csv('test.csv') #데이터 로드\n",
        "\n",
        "# 원본데이터, 사용자사전&강조&반복X\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200) #klue/roberta-large모델 사용, max seq lenght 200. 데이터 전처리 적용해 변환해옴\n",
        "pred1 = train_inference('klue/roberta-large',epocs=7,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred1') #원본 데이터로 학습한 결과\n",
        "#Graph Execution error로 인해 batchsize 조정\n",
        "\n",
        "with open('final_result/pred1.pickle', 'wb') as f:\n",
        "    pickle.dump(pred1, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "del pred1\n",
        "\n",
        "tf.keras.backend.clear_session() #세션 초기화\n",
        "\n",
        "\n",
        "# 원본데이터, 사용자사전&강조&반복O\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
        "pred2 = train_inference('klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred2')\n",
        "with open('final_result/pred2.pickle', 'wb') as f:\n",
        "    pickle.dump(pred2, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred2\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX7L02psRK-D"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train(adj,50).csv')   #형용사만 사용 증강\n",
        "my_seed_everywhere(42)\n",
        "\n",
        "\n",
        "# 형용사 이용 증강, 사용자사전&강조&반복X\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200) #전처리된 데이터를  받아옴\n",
        "pred3 = train_inference('klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=False,name = 'pred3')\n",
        "with open('final_result/pred3.pickle', 'wb') as f:\n",
        "    pickle.dump(pred3, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred3\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# 형용사 이용 증강, 사용자사전&강조&반복O\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
        "pred4 = train_inference('klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred4')\n",
        "with open('final_result/pred4.pickle', 'wb') as f:\n",
        "    pickle.dump(pred4, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred4\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTSqL-MuRK-D"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train(ad,50).csv')   #부사만 사용 증강\n",
        "\n",
        "# 부사만 이용 증강, 사용자사전&강조&반복X\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
        "pred5 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=False,name = 'pred5')\n",
        "with open('final_result/pred5.pickle', 'wb') as f:\n",
        "    pickle.dump(pred5, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred5\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# 부사만 이용 증강, 사용자사전&강조&반복O\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
        "pred6 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred'6)\n",
        "with open('final_result/pred6.pickle', 'wb') as f:\n",
        "    pickle.dump(pred6, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred6\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huR4kchwRK-D"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train(adj,ad,50).csv')   #형용사&부사 사용 증강\n",
        "\n",
        "my_seed_everywhere(42)\n",
        "# 형용사&부사 이용 증강, 사용자사전&강조&반복X\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
        "pred7 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred7')\n",
        "with open('final_result/pred7.pickle', 'wb') as f:\n",
        "    pickle.dump(pred7, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred7\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# 형용사&부사 이용 증강, 사용자사전&강조&반복O\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=False,token_change=True)\n",
        "pred8 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred8')\n",
        "with open('final_result/pred8.pickle', 'wb') as f:\n",
        "    pickle.dump(pred8, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred8\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgpMuYnNRK-D"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train(adj,ad,sim,100).csv')   #형용사&부사&유의어 사용 증강\n",
        "my_seed_everywhere(42)\n",
        "\n",
        "# 형용사&부사 이용 증강, 사용자사전&강조&반복X\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
        "pred9 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred9')\n",
        "with open('final_result/pred9.pickle', 'wb') as f:\n",
        "    pickle.dump(pred9, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred9\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# 형용사&부사 이용 증강, 사용자사전&강조&반복O\n",
        "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
        "pred10 = train_inference('klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred10')\n",
        "with open('final_result/pred10.pickle', 'wb') as f:\n",
        "    pickle.dump(pred10, f, pickle.HIGHEST_PROTOCOL)\n",
        "del pred10\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYl9BsZxRK-D"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "y_train = train['cat3']\n",
        "\n",
        "\n",
        "y_train = encoder.fit_transform(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00z62UoZRK-E"
      },
      "outputs": [],
      "source": [
        "# Ensemble\n",
        "def mode(list): #최빈값 구하기 위한 함수\n",
        "    count = 0\n",
        "    mode = 0\n",
        "    for x in list:\n",
        "        if list.count(x) > count:\n",
        "            count = list.count(x)\n",
        "            mode = x\n",
        "\n",
        "    return mode\n",
        "\n",
        "\n",
        "\n",
        "pred_ls = []\n",
        "for i in range(len(pred1)):\n",
        "  index = mode([ #voting\n",
        "      pred2[i].argmax(),\n",
        "      pred1[i].argmax(),\n",
        "      pred3[i].argmax(),\n",
        "      pred4[i].argmax(),\n",
        "      pred5[i].argmax(),\n",
        "      pred6[i].argmax(),\n",
        "      pred7[i].argmax(),\n",
        "      pred8[i].argmax(),\n",
        "      pred9[i].argmax(),\n",
        "      pred10[i].argmax(),\n",
        "      ])\n",
        "  pred_ls.append(encoder.classes_[index])\n",
        "y_pre= encoder.transform(pred_ls) #예측 결과에 대한 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPr5-Xr-RK-E"
      },
      "outputs": [],
      "source": [
        "# Image Model\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Rmkw-cRK-E"
      },
      "outputs": [],
      "source": [
        "# resize, Padding\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "def padding(img, set_size):\n",
        "\n",
        "    try:\n",
        "        h,w,c = img.shape\n",
        "    except:\n",
        "        print('파일을 확인후 다시 시작하세요.')\n",
        "        raise\n",
        "\n",
        "    if h < w: #가로가 더 긴 경우\n",
        "        new_width = set_size\n",
        "        new_height = int(new_width * (h/w)) #비율 유지 위함\n",
        "    else:\n",
        "        new_height = set_size\n",
        "        new_width = int(new_height * (w/h))\n",
        "\n",
        "    if max(h, w) < set_size: #지정한 사이즈보다 작을 때\n",
        "        img = cv2.resize(img, (new_width, new_height), cv2.INTER_CUBIC)\n",
        "    else: #지정 사이즈보다 클때\n",
        "        img = cv2.resize(img, (new_width, new_height), cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "    try:\n",
        "        h,w,c = img.shape\n",
        "    except:\n",
        "        print('파일을 확인후 다시 시작하세요.')\n",
        "        raise\n",
        "\n",
        "    delta_w = set_size - w\n",
        "    delta_h = set_size - h\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "\n",
        "    new_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]) #부족한 픽셀만큼 제로 패딩 적용\n",
        "\n",
        "    return new_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbqjDRNqRK-E"
      },
      "outputs": [],
      "source": [
        "#생성한 함수를 활용하여 학습데이터 생성\n",
        "\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "img_ls = train['img_path'].to_list()\n",
        "Timg_ls = test['img_path'].to_list()\n",
        "\n",
        "\n",
        "x_train = []\n",
        "\n",
        "for i in tqdm(range(len(img_ls))) :\n",
        "  img = cv2.imread(img_ls[i], cv2.IMREAD_COLOR)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pad_img = padding(img,IMAGE_SIZE[0]) #패딩 및 resize 적용\n",
        "  x_train.append(pad_img) #처리된 이미지 추가\n",
        "\n",
        "x_train = np.array(x_train) #numpy 배열 형태로 변환\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_test = []\n",
        "\n",
        "for i in tqdm(range(len(Timg_ls))) :\n",
        "  img = cv2.imread(Timg_ls[i], cv2.IMREAD_COLOR)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pad_img = padding(img,IMAGE_SIZE[0])\n",
        "  x_test.append(pad_img)\n",
        "\n",
        "x_test = np.array(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le6GWgPBRK-E"
      },
      "outputs": [],
      "source": [
        "pretrained_model = tf.keras.applications.RegNetY120(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3]) #top layer 제거해 모델 객체 생성\n",
        "# pretrained_model = efn.EfficientNetB7(weights='noisy-student', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "# pretrained_model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "#pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "#pretrained_model = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "#pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "# pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "# pretrained_model = tf.keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "pretrained_model.trainable = False # False = transfer learning, True = fine-tuning\n",
        "\n",
        "\n",
        "len(pretrained_model.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOkUR4NJRK-E"
      },
      "outputs": [],
      "source": [
        "# 이미지 모델 학습 함수 생성\n",
        "\n",
        "def train_inference(pretrained_model,open=-50,dim=64,dropout=0.2,optimizer='Adam',epocs=100,batch_size=8,sam=False):\n",
        "    for layer in pretrained_model.layers[:open]:      #전이학습 개방정도. open값 절대값 높을수록 더 많은 레이어를 학습시키게 됨\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        pretrained_model, #특성맵 추출\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),#GAP layer\n",
        "        tf.keras.layers.Dense(dim,activation='relu'), #dim만큼의 middle output 채널 수 가짐\n",
        "        tf.keras.layers.Dropout(dropout), #드롭아웃 적용\n",
        "        tf.keras.layers.Dense(128, activation='softmax') #128만큼의 output dim 가짐\n",
        "    ])\n",
        "\n",
        "    if sam == True :\n",
        "        model = SAMModel(model)\n",
        "    model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "    early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,restore_best_weights=True) #early stopping 적용 위한 객체 선언\n",
        "    model.fit(x_train, y_train, epochs=epocs, batch_size=batch_size, validation_split=0.1,callbacks=[early]) #모델 학습\n",
        "\n",
        "    pred = model.predict(x_test, batch_size=batch_size) #예측\n",
        "    return pred #예측결과 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyDivK8wRK-F"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-6) #Nadam optimizer\n",
        "pretrained_model = tf.keras.applications.RegNetY120(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "pretrained_model.trainable = False #전이학습\n",
        "pred11 = train_inference(pretrained_model,open=-1,dim=64,dropout=0.2,optimizer=optimizer,epocs=500,batch_size=8,sam=True) #극소수 레이어만 학습가능하도록 함\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "pretrained_model = tf.keras.applications.RegNetY120(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "pretrained_model.trainable = True\n",
        "pred12 = train_inference(pretrained_model,open=-50,dim=128,dropout=0.3,optimizer=optimizer,epocs=500,batch_size=8,sam=False)\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "pretrained_model = tf.keras.applications.RegNetY120(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "pretrained_model.trainable = True\n",
        "pred13 = train_inference(pretrained_model,open=-100,dim=128,dropout=0.3,optimizer=optimizer,epocs=500,batch_size=8,sam=False)\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI1XLrabRK-F"
      },
      "outputs": [],
      "source": [
        "def mode(list):\n",
        "    count = 0\n",
        "    mode = 0\n",
        "    for x in list:\n",
        "        if list.count(x) > count:\n",
        "            count = list.count(x)\n",
        "            mode = x\n",
        "\n",
        "    return mode\n",
        "\n",
        "\n",
        "\n",
        "pred_ls = []\n",
        "for i in range(len(pred1)):\n",
        "  index = mode([\n",
        "      pred2[i].argmax(),\n",
        "      pred1[i].argmax(),\n",
        "      pred3[i].argmax(),\n",
        "      pred4[i].argmax(),\n",
        "      pred5[i].argmax(),\n",
        "      pred6[i].argmax(),\n",
        "      pred7[i].argmax(),\n",
        "      pred8[i].argmax(),\n",
        "      pred9[i].argmax(),\n",
        "      pred10[i].argmax(),\n",
        "      pred11[i].argmax(),\n",
        "      pred12[i].argmax(),\n",
        "      pred13[i].argmax(),\n",
        "      ])\n",
        "  pred_ls.append(encoder.classes_[index])\n",
        "y_pre2= encoder.transform(pred_ls)\n",
        "\n",
        "# 텍스트 + 이미지 앙상블 결과 : y_pre2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJPC06x_RK-F"
      },
      "source": [
        "1. 이렇게 텍스트만 사용한 y_pre\n",
        "2. 텍스트 이미지 머신러닝을 사용한 y_pre2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otJUEoSxRK-K"
      },
      "outputs": [],
      "source": [
        "# 1. y_pre를 소분류로 변형하여 csv로 만들어 저장합니다.\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "y_train = train_set['cat3']\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "\n",
        "\n",
        "cat3 = encoder.inverse_transform(y_pre)\n",
        "df_cat3 = pd.DataFrame({'cat3':cat3})\n",
        "test = test['id']\n",
        "sample_submission = pd.concat([test,df_cat3],axis=1)\n",
        "sample_submission.to_csv('result1.csv',index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq0UFC7QRK-L"
      },
      "outputs": [],
      "source": [
        "# 2. y_pre2를 소분류로 변형하여 csv로 만들어 저장합니다.\n",
        "\n",
        "cat3 = encoder.inverse_transform(y_pre2)\n",
        "df_cat3 = pd.DataFrame({'cat3':cat3})\n",
        "sample_submission = pd.concat([test,df_cat3],axis=1)\n",
        "sample_submission.to_csv('result2.csv',index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzTYJZULTSqc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsdjv9zBTiiA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y4wo6fmK02M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "865f927eac30e896ebe15021531e7e53375d7d4c435427f165fd8b672885c786"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
